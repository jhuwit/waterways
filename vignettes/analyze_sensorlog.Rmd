---
title: "Analysis of SensorLog and GT3X Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Analysis of SensorLog and GT3X Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: refs.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(dplyr)
library(waterways)
library(SummarizedActigraphy)
library(read.gt3x)
library(lubridate)
library(tidyr)
library(sf)
library(ggplot2)
```

  
# Introduction
In this vignette, we will go through the steps on how to analyze SensorLog data.  We will go through the steps of reading in the data, checking for potential duplicates of timestamps, calculating distance metrics from a fixed point (commonly participant home), and creating daily summaries.  
  
We will also integrate ActiGraph data to show activity at different distances.
  
# Methods
## Data
The data used is example data embedded in the waterways package. It is a single participant for approximately two hours worth of GPS and activity data. The SensorLog data is stored in a zip file and contains a single CSV file. The data is stored in the `extdata` folder of the package.  We can get the path of the file on each user's machines using `system.file` and the package name:
  
```{r sensorlog_file}
library(waterways)
file = system.file(
  "extdata", "SensorLogFiles_my_iOS_device_250311_14-55-58.zip",
  package = "waterways")
file
```

This can also be done using the wrapper from `waterways`:
```{r ww_sensorlog_file}
ww_example_sensorlog_file()
```

### Reading in the data
The data is read in using the `ww_read_sensorlog` function:

```{r read_sensorlog}
library(dplyr)
df = ww_read_sensorlog(file, robust = FALSE)
df = df %>% select(-file) # we don't need to see which file this came from
head(df)
```

The option `robust` will run `rewrite_sensorlog_csv` is run on the data.  The `rewrite_sensorlog_csv` attempts to fix any shifts with the data, which have been seen due to a bug in the iOS app.  The option default is `FALSE` because this can take a much longer time to read in the data due to reading and writing the data back out to a temporary file. This function will also flag any values of latitude and longitude that are 0 (absolute value $< 0.00001$).  This flag is created because the iOS app will sometimes record 0 for latitude and longitude when the GPS signal is lost. 

We see that the data has the following columns, which are renamed from the original data.  
```{r}
colnames(df)
```
The mapping from the new column names from the original column names can be seen using `ww_sensorlog_csv_colnames_mapping`:
```{r}
ww_sensorlog_csv_colnames_mapping()
```


### Processing the data
The data is then processed using the `ww_process_sensorlog` function. This function (currently) will:

1. Check the data for duplicated data using `ww_check_data`.
2. Calculate distance from a central point if provided using `ww_calculate_distance`
3. Process the timing data using `ww_process_time`, including checking for duplicate times.  This function will also determine if the data is in the correct timezone by estimating the timezone expected (in the `expected_timezone` field, set to `America/New_York`) based the GPS coordinates, using the `lutz::tz_lookup_coords` function.  If this check is not desired, set `expected_timezone = NULL`:

```{r process_sensorlog_fail, error=TRUE}
ww_process_sensorlog(df)
```

As the error indicates, there are some duplicate times in this file. We can look at those specific duplicate times and see that even to 3 digits for milliseconds, there are multiple measurements:

```{r}
df %>% 
  add_count(time) %>% 
  filter(n > 1) %>% 
  select(time, lat, lon, starts_with("accel"))
```

Thus, we do not want to ensure no duplicated times, so we will set `check_data` to be `FALSE`:

```{r process_sensorlog}
df_proc = ww_process_sensorlog(df, check_data = FALSE, apply_tz = TRUE)
df_proc
```

Note, the argument `apply_tz` will apply the time zone of the data.  This is helpful as it will set the correct time zone of the data in absolute terms.  Unfortunately, this makes it so researchers have to make sure they time shift things like hours.  For example, if you want to find what data is between 10AM and 6PM in EST time zone, you need to shift those to GMT/UTC.  Also, this will cause issues as it shifts data into different dates. The data is likely/sleep rest, but this can skew numbers.  For analysis, we will not apply the timezone:

```{r process_sensorlog_for_real}
df = ww_process_sensorlog(df, check_data = FALSE, apply_tz = FALSE)
df
```



The data times will be projected into the `GMT` timezone to agree with the output from ActiGraph and `read.gt3x::read.gt3x`. **NOTE:** this shifts times from the original timezone.  For example, if the data was at "2025-03-25 14:14:00" with a timezone of `-04:00` then the data will be in the `GMT` timezone, but the printed time will be "2025-03-25 18:14:00 GMT".  You can set the timezone to the correct timezone using `lubridate::with_tz` or `lubridate::force_tz` (they have different behavior).

You can also pass in the `tz = "EST"` argument to `ww_process_sensorlog`, which is passed to `ww_process_time`.  An additional column of `char_time` is added so that the original character values fo the time are retained so that users can check any timezone conversions, as they can lead to issues with merging data, determining windows (e.g. wake), or other time-based analyses.

```{r}
df %>% 
  select(time, char_time, timestamp) %>% 
  head()
```



### Calculating distance
We did not pass in the latitude and longitude of a fixed point to calculate distance for the processing function, but we could have. We can also calculate the distance separately using the function `ww_calculate_distance`.  First, we need to get the latitude and longitude of the fixed point.  We can use the `tidygeocoder` package to get the latitude and longitude of a fixed point.  We will use the address `615 N Wolfe St, Baltimore MD` for the Johns Hopkins Bloomberg School of Public Health where this data was collected, using the [Census Geocoder](https://geocoding.geo.census.gov/geocoder/) for free:

```{r geo_show, eval = FALSE}
geo = tidygeocoder::geo("615 N Wolfe St, Baltimore MD", method = "census")
print(geo)
lat = geo$lat
lon = geo$long
```

```{r geo_run, echo = FALSE}
geo = structure(list(
  address = "615 N Wolfe St, Baltimore MD", lat = 39.297622492942, 
  long = -76.590753361728), class = c("tbl_df", "tbl", "data.frame"
  ), row.names = c(NA, -1L))
print(geo)
lat = geo$lat
lon = geo$long
```

We can also get Census-level information using:
```{r, cache = TRUE}
df_census = data.frame(street = "615 N Wolfe St", 
                       city = "Baltimore", state = "MD",
                       zip = "21205") %>% 
  censusxy::cxy_geocode(
    street = "street",
    city = "city", 
    state = "state",
    zip = "zip",
    output = "full",
    return = "geographies",
    benchmark = "Public_AR_Current",
    vintage = "Census2010_Current")
df_census
```

From this, we can construct a 12-digits FIPS code or `GEOID10` which is the census tract required for geocoding the EPA walkability index:

```{r get_epa}
if (rlang::is_installed("arcgislayers")) {
  df_census = df_census %>% 
    mutate(GEOID10 = ww_fips12(cxy_state_id, cxy_county_id, cxy_tract_id, cxy_block_id))
  epa_walkability = ww_epa_walkability(geoid = df_census$GEOID10)
  print(epa_walkability)
}
```

We can simply focus on the EPA walkability index, which is the `NatWalkInd` column.  We can also get the EPA walkability index category breaks from the `cat_walk_index`:

```{r get_epa_cat}
if (rlang::is_installed("arcgislayers")) {
  print(epa_walkability %>% 
          as.data.frame() %>% 
            select(NatWalkInd, cat_walk_index))
}
```



We can then calculate the distance from this fixed point using the `ww_calculate_distance` function:

```{r calc_distance}
df = ww_calculate_distance(df,
                           lat = lat,
                           lon = lon)
```

This function uses the `geosphere::distm` function to calculate the distance in meters from the fixed point.  We can see the new columns that were added to the data.  



```{r}
df %>% 
  select(time, char_time, lat_zero, lon_zero, distance)
```

## Summarizing the Data
Now that we have the distance for each point in time, we can summarize the data.  In many cases, we do not need the data at a sub-second or second-level.  We can summarize the data at a daily level using the `ww_summarize_sensorlog` function.  This function will take the mean over a period of seconds, usually 60:

```{r summarize_sensorlog_minute}
df_min = ww_minute_sensorlog(df)
head(df_min)
```

We can also summarize the data at a daily level using the `ww_summarize_sensorlog` function:

```{r summarize_sensorlog}
df_sum = ww_summarize_sensorlog(df)
df_sum
```

One of the main issues with the daily level if you typically want to cross-referene GPS with activity, so we will keep the data at a minute level for now.


## Reading in the GT3X data
To summarize the activity profile for different levels of proximity to a fixed point we will read in the GT3X data. This data was captured using an ActiGraph (Pensacola FL) GT9X device. The data is stored in the `extdata` folder of the package, similar to the SensorLog data. We get the path via `ww_example_gt3x_file`:

```{r ag_file}
file_gt3x = ww_example_gt3x_file()
file_gt3x
```


We can read in the data using the `read.gt3x` package, but waterways wraps this using `ww_read_gt3x`.


```{r ag_read}
ag_tz_applied = ww_read_gt3x(file_gt3x, verbose = FALSE, apply_tz = TRUE)
head(ag_tz_applied)
```

We can check the timezone of the data using the `lubridate::tz` function:
```{r ag_tz}
lubridate::tz(ag_tz_applied$time)
```

The `apply_tz` argument in `ww_read_gt3x` (by default) will find the timezone in the header of the file, and apply it to the data.  This is done using the `lubridate::with_tz` function.  The timezone is stored in the header of the file, which can be seen using the header attribute:

We can confirm this via the header:
```{r}
header = attributes(ag_tz_applied)$header
header
header$TimeZone
```

The timezone is stored in the `TimeZone` field. So the data is in GMT, but it is "correct" since this is the local time **projected** into GMT.  For example, if you measured data in a timezone of `-04:00` and `read.gt3x` gave you a value of `2025-03-25 14:14:00`, then the *true* time in GMT would be `2025-03-25 18:14:00`.  Using `read.gt3x`, the time incorrectly would be `2025-03-25 14:14:00 GMT`, but with `ww_read_gt3x` it would be `2025-03-25 18:14:00 GMT`.  This means that if you want convert timezones, you will need to use `lubridate::with_tz` or `lubridate::force_tz`.

We will not apply the timezone information similarly as the SensorLog for the same reasons we discussed above.  We can read in the data without applying the timezone, giving "incorrect" data but that can be merged with SensorLog and does not shift times to different dates:

```{r ag_read2}
ag = ww_read_gt3x(file_gt3x, verbose = FALSE, apply_tz = FALSE)
head(ag)
lubridate::tz(ag$time)
```


In ActiLife, the ActiGraph software, values of zero for all axes are set to the last observation carried forward (LOCF).  The `fill_zeroes` argument will (by default) apply this.  Also, when the data is read in using `read.gt3x`:
>  DateTimes are therefore represented as POSIXct format with the 'GMT' timezone attribute, which is false; the datetime actually represents local time.


We can calculate a number of summary measures from the acceleration.  We will calculate ActiGraph Activity Counts using the `agcounts` package.  We will use the `epoch = 60` to calculate the counts in 60-second epochs. From these counts, we can estimate non-wear using the Choi [@choi2011validation] algorithm, which we use from the `actigraph.sleepr` package.  The `ww_process_gt3x` will perform these operations together:


```{r calculate_counts}
counts = ww_process_gt3x(ag, verbose = FALSE)
counts %>% 
  head()
counts = counts %>% select(time, counts, wear)
```

### Overlap with SensorLog data
The SensorLog data used was a subset of the data from the ActiGraph data and multiple recordings were done.  We can see the overlap by using the `time` column is used to find the overlap.  

```{r}
data = counts %>% 
  mutate(in_counts = TRUE) %>% 
  full_join(df_min %>% 
             mutate(in_sensorlog = TRUE),
            by = join_by(time)) %>% 
  tidyr::replace_na(list(in_counts = FALSE, in_sensorlog = FALSE))
data = data %>% 
  filter(in_counts & in_sensorlog)
data
```

Here we see for this data that there is complete wear:

```{r}
all(data$wear)
```


Here we can look at the counts within "home"/work from those without (there are none) coupled with the variant that takes into account wear time:
```{r}
data %>% 
  group_by(is_within_home) %>% 
  summarise(
    sum_counts = sum(counts),
    mean_counts = mean(counts),
    sum_counts_wear = sum(counts[wear]),
    mean_counts_wear = mean(counts[wear])
  )
```

### Getting Times
 You can easily manipulate the time variables to get the date, hour, and minute level data.  Here is an example function `create_date_hour_minute`.  We recommend using the `hms` data type for the hour and minute columns.  since you many times want to plot the data by hour/minute and facet by day:

```{r}
create_date_hour_minute = function(data) {
  data = data %>%
    mutate(
      date = lubridate::floor_date(time, "1 day"),
      date = lubridate::as_date(date),
      hour = lubridate::floor_date(time, "1 hour"),
      hour = hms::as_hms(hour),
      minute = lubridate::floor_date(time, "1 minute"),
      minute = hms::as_hms(minute)
    )
}
```


Alternatively, if you group the data by participant, you can create an observation `day` variable that puts data on the same per-person time frame (time from start) versus calendar time:
```{r}
create_day = function(data) {
  data = data %>%
    dplyr::mutate(
      day = as.numeric(difftime(date, min(date), units = "days") + 1)
    )
}
```


We can plot the counts over time, colored by within home/work (all is within that):
```{r}
data %>% 
  create_date_hour_minute() %>% 
  ggplot(aes(x = minute, y = counts)) +
  geom_point(aes(colour = is_within_home)) + 
  geom_line() + 
  facet_wrap( ~ date)
```

And we can also color by distance from home, so not much difference here:
```{r}
data %>% 
  create_date_hour_minute() %>% 
  ggplot(aes(x = minute, y = counts)) +
  geom_point(aes(colour = distance)) + 
  geom_line() + 
  facet_wrap( ~ date)
```


### Estimating Wear Time

We can estimate wear time using the `wear` column from the ActiGraph data.  Here we will calculate the sum of the wear at each date:

```{r}
data %>% 
  create_date_hour_minute() %>% 
  group_by(date) %>% 
  summarise(
    n = n(),
    n_wear = sum(wear),
    n_nonwear = sum(!wear)
  )
```


Without regard to SensorLog data, we can calculate the wear time by date.  This is useful if you want to see how much data was collected on a given day:

```{r}
counts %>% 
  create_date_hour_minute() %>% 
  group_by(date) %>% 
  summarise(
    n = n(),
    n_wear = sum(wear),
    n_nonwear = sum(!wear)
  )
```

Once this is calculated, you can exclude dates without insufficient wear time (e.g. 95% of the day or 1368 minutes, or 10 hours of wear).  Different tasks require different criteria for wear time.




# References
